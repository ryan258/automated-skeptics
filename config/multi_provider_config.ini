# automated_skeptic_mvp/config/multi_provider_config.ini
# Configuration with ALL LLM providers: OpenAI, Claude, Gemini, Ollama

[API_KEYS]
# OpenAI API key
openai_api_key = your_openai_api_key_here

# Anthropic/Claude API key
anthropic_api_key = your_anthropic_api_key_here

# Google AI/Gemini API key
google_ai_api_key = your_google_ai_api_key_here

# News and search APIs (existing)
news_api_key = your_news_api_key_here
google_search_api_key = your_google_search_api_key_here
google_search_engine_id = your_search_engine_id_here

[API_SETTINGS]
request_timeout = 30
max_retries = 3
rate_limit_delay = 1.0

[PROCESSING]
max_sources_per_claim = 5
confidence_threshold = 0.7
cache_expiry_hours = 24

[LLM_MODELS]
# === OPENAI CONFIGURATION ===
openai_model = gpt-4o-mini
openai_temperature = 0.1
openai_max_tokens = 500

# === CLAUDE/ANTHROPIC CONFIGURATION ===
claude_model = claude-3-5-sonnet-20241022
claude_temperature = 0.1
claude_max_tokens = 500

# === GEMINI/GOOGLE AI CONFIGURATION ===
gemini_model = gemini-1.5-flash
gemini_temperature = 0.1
gemini_max_tokens = 500

# === OLLAMA CONFIGURATION ===
ollama_enabled = true
ollama_model = llama2:latest
ollama_base_url = http://localhost:11434
ollama_temperature = 0.1
ollama_max_tokens = 500

[AGENT_LLM_MAPPING]
# STRATEGY 1: LOCAL-FIRST (Cost Effective)
# Use local models for most tasks, external for complex reasoning

herald_llm = ollama
herald_model = phi3:latest
herald_temperature = 0.0
herald_max_tokens = 200

illuminator_llm = ollama
illuminator_model = llama3.2:latest
illuminator_temperature = 0.1
illuminator_max_tokens = 300

logician_llm = claude  # Complex reasoning - use best external model
logician_model = claude-3-5-sonnet-20241022
logician_temperature = 0.1
logician_max_tokens = 600

seeker_llm = ollama
seeker_model = llama3.2:latest
seeker_temperature = 0.0
seeker_max_tokens = 200

oracle_llm = claude  # Critical analysis - use best external model
oracle_model = claude-3-5-sonnet-20241022
oracle_temperature = 0.1
oracle_max_tokens = 800

# STRATEGY 2: QUALITY-FIRST (Uncomment to use)
# Use best external models for everything
# herald_llm = gemini
# herald_model = gemini-1.5-flash
# illuminator_llm = gemini
# illuminator_model = gemini-1.5-flash
# logician_llm = claude
# logician_model = claude-3-5-sonnet-20241022
# seeker_llm = gemini
# seeker_model = gemini-1.5-flash
# oracle_llm = claude
# oracle_model = claude-3-5-sonnet-20241022

# STRATEGY 3: BALANCED (Uncomment to use)
# Use different providers for different strengths
# herald_llm = ollama          # Fast local processing
# herald_model = phi3:latest
# illuminator_llm = gemini     # Good at classification
# illuminator_model = gemini-1.5-flash
# logician_llm = claude        # Best reasoning
# logician_model = claude-3-5-sonnet-20241022
# seeker_llm = openai          # Good at search planning
# seeker_model = gpt-4o-mini
# oracle_llm = claude          # Best for final analysis
# oracle_model = claude-3-5-sonnet-20241022

[COST_SETTINGS]
# Prefer local models when available
prefer_local = true

# Maximum daily cost threshold (USD)
max_daily_cost = 10.0

# Use local models for development/testing
development_mode = true

[PERFORMANCE]
# Enable parallel LLM processing where possible
enable_parallel_processing = false

# Timeout settings per provider type
local_llm_timeout = 60
external_llm_timeout = 30

# Enable LLM response caching
enable_llm_caching = true
llm_cache_expiry = 24

# Provider-specific settings
[OPENAI_SETTINGS]
# Use newer models if available
use_latest_models = true

[CLAUDE_SETTINGS]
# Claude-specific optimizations
use_system_prompts = true

[GEMINI_SETTINGS]
# Gemini-specific optimizations
safety_settings = default

[OLLAMA_SETTINGS]
# Ollama-specific optimizations
keep_models_loaded = true
gpu_acceleration = true