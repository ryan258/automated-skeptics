# automated_skeptic_mvp/config/optimized_config.ini
# OPTIMIZED configuration using your working multi-provider setup

[API_KEYS]
# Your working API keys (don't change these)
openai_api_key = your_openai_api_key_here
anthropic_api_key = your_anthropic_api_key_here
google_ai_api_key = your_google_ai_api_key_here

# News and search APIs
news_api_key = your_news_api_key_here
google_search_api_key = your_google_search_api_key_here
google_search_engine_id = your_search_engine_id_here

[API_SETTINGS]
request_timeout = 30
max_retries = 3
rate_limit_delay = 1.0

[PROCESSING]
max_sources_per_claim = 5
confidence_threshold = 0.7
cache_expiry_hours = 24

[LLM_MODELS]
# === OPENAI CONFIGURATION ===
openai_model = gpt-4o-mini
openai_temperature = 0.1
openai_max_tokens = 500

# === CLAUDE/ANTHROPIC CONFIGURATION ===
claude_model = claude-3-5-sonnet-20241022
claude_temperature = 0.1
claude_max_tokens = 500

# === GEMINI/GOOGLE AI CONFIGURATION ===
gemini_model = gemini-1.5-flash
gemini_temperature = 0.1
gemini_max_tokens = 500

# === OLLAMA CONFIGURATION ===
ollama_enabled = true
ollama_model = llama2:latest
ollama_base_url = http://localhost:11434
ollama_temperature = 0.1
ollama_max_tokens = 500

[AGENT_LLM_MAPPING]
# ðŸš€ OPTIMAL STRATEGY: Fast local + Premium cloud for complex tasks

# LIGHTWEIGHT TASKS - Use fast local models
herald_llm = ollama
herald_model = phi3:latest
herald_temperature = 0.0
herald_max_tokens = 200

illuminator_llm = ollama
illuminator_model = llama3.2:latest
illuminator_temperature = 0.1
illuminator_max_tokens = 300

seeker_llm = ollama
seeker_model = llama3.2:latest
seeker_temperature = 0.0
seeker_max_tokens = 200

# COMPLEX REASONING - Use premium models
# Logician: Complex claim deconstruction (use BEST reasoning model)
logician_llm = claude
logician_model = claude-3-5-sonnet-20241022
logician_temperature = 0.1
logician_max_tokens = 600

# Oracle: Critical evidence analysis (use BEST analysis model)
oracle_llm = claude
oracle_model = claude-3-5-sonnet-20241022
oracle_temperature = 0.1
oracle_max_tokens = 800

# ALTERNATIVE STRATEGY: Speed-optimized
# Uncomment these to use Gemini for fast premium processing
# logician_llm = gemini
# logician_model = gemini-1.5-flash
# oracle_llm = gemini  
# oracle_model = gemini-1.5-flash

# ALTERNATIVE STRATEGY: Cost-optimized
# Uncomment these to use everything local (free but slower)
# logician_llm = ollama
# logician_model = deepseek-r1:14b
# oracle_llm = ollama
# oracle_model = llama3.1:latest

[COST_SETTINGS]
# Prefer local models when available
prefer_local = true

# Maximum daily cost threshold (USD)
max_daily_cost = 5.0

# Use local models for development/testing
development_mode = false  # Set to true to force local-only

[PERFORMANCE]
# Optimize for your multi-provider setup
enable_parallel_processing = false
local_llm_timeout = 60
external_llm_timeout = 30
enable_llm_caching = true
llm_cache_expiry = 24

# Provider-specific optimizations
[PROVIDER_PREFERENCES]
# Fallback order for external providers
external_fallback_order = claude, gemini, openai

# Use fastest external provider for time-sensitive tasks
speed_optimized_provider = gemini

# Use best reasoning provider for complex analysis
reasoning_optimized_provider = claude