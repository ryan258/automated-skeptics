# automated_skeptic_mvp/config/config_optimized.ini
# Optimized configuration using your available models

[API_KEYS]
openai_api_key = 
news_api_key = your_news_api_key_here
google_search_api_key = your_google_search_api_key_here
google_search_engine_id = your_search_engine_id_here

[API_SETTINGS]
request_timeout = 30
max_retries = 3
rate_limit_delay = 1.0

[PROCESSING]
max_sources_per_claim = 5
confidence_threshold = 0.7
cache_expiry_hours = 24

[LLM_MODELS]
openai_model = gpt-4o-mini
openai_temperature = 0.1
openai_max_tokens = 500

ollama_enabled = true
ollama_model = llama2:latest
ollama_base_url = http://localhost:11434
ollama_temperature = 0.1
ollama_max_tokens = 500

[AGENT_LLM_MAPPING]
# Optimized model assignments using your available models

herald_llm = ollama
herald_model = phi3:latest
herald_temperature = 0.0
herald_max_tokens = 200

illuminator_llm = ollama
illuminator_model = llama3.2:latest
illuminator_temperature = 0.1
illuminator_max_tokens = 300

logician_llm = ollama
logician_model = deepseek-r1:14b
logician_temperature = 0.1
logician_max_tokens = 500

seeker_llm = ollama
seeker_model = llama3.2:latest
seeker_temperature = 0.0
seeker_max_tokens = 200

oracle_llm = ollama
oracle_model = deepseek-r1:14b
oracle_temperature = 0.1
oracle_max_tokens = 600

[COST_SETTINGS]
prefer_local = true
max_daily_cost = 5.0
development_mode = true

[PERFORMANCE]
enable_parallel_processing = false
local_llm_timeout = 60
external_llm_timeout = 30
enable_llm_caching = true
llm_cache_expiry = 24