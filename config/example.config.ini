# automated_skeptic_mvp/config/example.config.ini

[API_KEYS]
# OpenAI API key for external LLM access
openai_api_key = your_openai_api_key_here

# NewsAPI key for news source integration
news_api_key = your_news_api_key_here

# Google Custom Search API
google_search_api_key = your_google_search_api_key_here
google_search_engine_id = your_search_engine_id_here

[API_SETTINGS]
# Request timeout in seconds
request_timeout = 30

# Maximum retries for failed requests
max_retries = 3

# Delay between requests to respect rate limits
rate_limit_delay = 1.0

[PROCESSING]
# Maximum number of sources to gather per claim
max_sources_per_claim = 5

# Confidence threshold for verdict decisions
confidence_threshold = 0.7

# Cache expiry time in hours
cache_expiry_hours = 24

[LLM_MODELS]
# === OPENAI CONFIGURATION ===
# Default OpenAI model to use
openai_model = gpt-3.5-turbo
# Temperature for OpenAI models (0.0 = deterministic, 1.0 = creative)
openai_temperature = 0.1
# Maximum tokens for OpenAI responses
openai_max_tokens = 500

# === OLLAMA CONFIGURATION ===
# Enable/disable Ollama (local LLM)
ollama_enabled = true
# Default Ollama model (make sure this model is installed)
ollama_model = llama2
# Ollama server URL
ollama_base_url = http://localhost:11434
# Temperature for Ollama models
ollama_temperature = 0.1
# Maximum tokens for Ollama responses
ollama_max_tokens = 500

# === AGENT-SPECIFIC LLM MAPPING ===
[AGENT_LLM_MAPPING]
# Specify which LLM each agent should use
# Options: openai, ollama, or leave blank for automatic selection

# Herald Agent - Input processing (lightweight task, good for local)
herald_llm = ollama
herald_model = llama2
herald_temperature = 0.0
herald_max_tokens = 200

# Illuminator Agent - Context analysis (medium complexity)
illuminator_llm = ollama
illuminator_model = llama2
illuminator_temperature = 0.1
illuminator_max_tokens = 300

# Logician Agent - Claim deconstruction (complex reasoning, may benefit from GPT)
logician_llm = openai
logician_model = gpt-3.5-turbo
logician_temperature = 0.1
logician_max_tokens = 500

# Seeker Agent - Research planning (lightweight, good for local)
seeker_llm = ollama
seeker_model = llama2
seeker_temperature = 0.0
seeker_max_tokens = 200

# Oracle Agent - Evidence synthesis (complex reasoning, may benefit from GPT)
oracle_llm = openai
oracle_model = gpt-3.5-turbo
oracle_temperature = 0.1
oracle_max_tokens = 600

# === COST OPTIMIZATION ===
[COST_SETTINGS]
# Prefer local models when available
prefer_local = true

# Maximum daily cost threshold (USD)
max_daily_cost = 5.0

# Use local models for development/testing
development_mode = true

# === PERFORMANCE TUNING ===
[PERFORMANCE]
# Enable parallel LLM processing where possible
enable_parallel_processing = true

# Timeout for local LLM requests (seconds)
local_llm_timeout = 60

# Timeout for external LLM requests (seconds)
external_llm_timeout = 30

# Enable LLM response caching
enable_llm_caching = true

# LLM cache expiry (hours)
llm_cache_expiry = 24