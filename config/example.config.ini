# automated_skeptic_mvp/config/example.config.ini
# Multi-Provider Configuration Template

[API_KEYS]
# === REQUIRED FOR MULTI-PROVIDER SETUP ===
# OpenAI API key
openai_api_key = your_openai_api_key_here

# Claude/Anthropic API key (Get from: https://console.anthropic.com/)
anthropic_api_key = your_anthropic_api_key_here

# Gemini/Google AI API key (Get from: https://aistudio.google.com/app/apikey)
google_ai_api_key = your_google_ai_api_key_here

# === OPTIONAL APIs ===
# NewsAPI key for news source integration
news_api_key = your_news_api_key_here
google_search_api_key = your_google_search_api_key_here
google_search_engine_id = your_search_engine_id_here

[LLM_MODELS]
# === OPENAI CONFIGURATION ===
openai_model = gpt-4o-mini
openai_temperature = 0.1
openai_max_tokens = 500

# === CLAUDE/ANTHROPIC CONFIGURATION ===
claude_model = claude-3-5-sonnet-20241022
claude_temperature = 0.1
claude_max_tokens = 500

# === GEMINI/GOOGLE AI CONFIGURATION ===
gemini_model = gemini-1.5-flash
gemini_temperature = 0.1
gemini_max_tokens = 500

# === OLLAMA CONFIGURATION ===
ollama_enabled = true
ollama_model = llama2:latest
ollama_base_url = http://localhost:11434
ollama_temperature = 0.1
ollama_max_tokens = 500

[AGENT_LLM_MAPPING]
# === OPTIMAL HYBRID STRATEGY ===
# Use local models for simple tasks, premium models for complex reasoning

# Lightweight processing - Local models (FREE)
herald_llm = ollama
herald_model = phi3:latest

illuminator_llm = ollama
illuminator_model = llama3.2:latest

seeker_llm = ollama
seeker_model = llama3.2:latest

# Complex reasoning - Premium models (BEST QUALITY)
logician_llm = claude
logician_model = claude-3-5-sonnet-20241022

oracle_llm = claude
oracle_model = claude-3-5-sonnet-20241022

# === ALTERNATIVE STRATEGIES ===
# Uncomment for different optimization approaches:

# SPEED-OPTIMIZED (Use Gemini for external tasks)
# logician_llm = gemini
# oracle_llm = gemini

# COST-OPTIMIZED (Use only local models)
# logician_llm = ollama
# logician_model = deepseek-r1:14b
# oracle_llm = ollama
# oracle_model = llama3.1:latest

[COST_SETTINGS]
prefer_local = true
max_daily_cost = 5.0
development_mode = false