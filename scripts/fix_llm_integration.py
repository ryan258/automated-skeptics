#!/usr/bin/env python3
# automated_skeptic_mvp/scripts/fix_llm_integration.py
"""
Fix script for LLM integration issues
"""

import subprocess
import sys
import os
import json
from pathlib import Path

def check_python_version():
    """Check if Python version is compatible"""
    if sys.version_info < (3, 8):
        print("‚ùå Python 3.8+ required. Current version:", sys.version)
        return False
    print(f"‚úÖ Python version: {sys.version.split()[0]}")
    return True

def update_openai_package():
    """Update OpenAI package to compatible version"""
    print("\nüîÑ Updating OpenAI package...")
    try:
        # Install the new OpenAI package
        result = subprocess.run([
            sys.executable, "-m", "pip", "install", "openai>=1.0.0", "--upgrade"
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("‚úÖ OpenAI package updated successfully")
            return True
        else:
            print(f"‚ùå Failed to update OpenAI: {result.stderr}")
            return False
    except Exception as e:
        print(f"‚ùå Error updating OpenAI package: {str(e)}")
        return False

def check_ollama_installation():
    """Check if Ollama is installed and running"""
    print("\nüîç Checking Ollama installation...")
    
    try:
        # Check if Ollama command exists
        result = subprocess.run(["ollama", "--version"], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"‚úÖ Ollama installed: {result.stdout.strip()}")
        else:
            print("‚ùå Ollama not found in PATH")
            return False
    except FileNotFoundError:
        print("‚ùå Ollama not installed")
        print("üí° Install from: https://ollama.ai/download")
        return False
    
    # Check if Ollama server is running
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            print(f"‚úÖ Ollama server running with {len(models)} models")
            if models:
                print("üì¶ Available models:")
                for model in models[:5]:  # Show first 5
                    print(f"   ‚Ä¢ {model['name']}")
                if len(models) > 5:
                    print(f"   ... and {len(models) - 5} more")
            return True
        else:
            print("‚ùå Ollama server not responding")
            return False
    except Exception as e:
        print("‚ùå Ollama server not accessible")
        print("üí° Start with: ollama serve")
        return False

def ensure_basic_models():
    """Ensure basic models are available in Ollama"""
    print("\nüì¶ Checking basic models...")
    
    recommended_models = ["llama2", "llama3.2"]
    
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            available_models = [model['name'] for model in response.json().get('models', [])]
            
            # Check if we have any suitable model
            suitable_models = []
            for model in available_models:
                for rec in recommended_models:
                    if model.startswith(rec):
                        suitable_models.append(model)
                        break
            
            if suitable_models:
                print(f"‚úÖ Found suitable models: {suitable_models}")
                return True
            else:
                print("‚ö†Ô∏è  No recommended models found")
                print("üí° Pull a model with: ollama pull llama2")
                return False
        
    except Exception as e:
        print(f"‚ùå Error checking models: {str(e)}")
        return False

def update_config_file():
    """Update configuration file with better model names"""
    print("\n‚öôÔ∏è  Updating configuration...")
    
    config_path = Path("config/config.ini")
    if not config_path.exists():
        print("‚ö†Ô∏è  Config file not found - using defaults")
        return True
    
    try:
        # Read current config
        with open(config_path, 'r') as f:
            content = f.read()
        
        # Update model references to include :latest tag
        updated_content = content.replace(
            "ollama_model = llama2",
            "ollama_model = llama2:latest"
        )
        
        # Update agent mappings if they exist
        agent_models = ["herald_model", "illuminator_model", "seeker_model"]
        for agent_model in agent_models:
            updated_content = updated_content.replace(
                f"{agent_model} = llama2",
                f"{agent_model} = llama2:latest"
            )
        
        # Write back if changed
        if updated_content != content:
            with open(config_path, 'w') as f:
                f.write(updated_content)
            print("‚úÖ Configuration updated")
        else:
            print("‚úÖ Configuration already up to date")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error updating config: {str(e)}")
        return False

def test_integration():
    """Test the LLM integration"""
    print("\nüß™ Testing LLM integration...")
    
    try:
        # Import and test the components
        sys.path.append('.')
        
        from config.settings import Settings
        from llm.manager import LLMManager
        
        settings = Settings()
        llm_manager = LLMManager(settings)
        
        providers = llm_manager.get_available_providers()
        print(f"‚úÖ LLM Manager initialized with {len(providers)} providers")
        
        for name, info in providers.items():
            status = "‚úÖ" if info['available'] else "‚ùå"
            print(f"   {status} {name}: {info['provider']} - {info['model']}")
        
        if providers:
            # Test a simple generation
            try:
                response = llm_manager.generate("Hello, this is a test.")
                print(f"‚úÖ Test generation successful: {response.content[:50]}...")
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è  Generation test failed: {str(e)}")
                return len(providers) > 0  # At least providers are available
        else:
            print("‚ùå No providers available")
            return False
            
    except Exception as e:
        print(f"‚ùå Integration test failed: {str(e)}")
        return False

def main():
    """Main fix script"""
    print("üîß AUTOMATED SKEPTIC - LLM INTEGRATION FIX SCRIPT")
    print("=" * 60)
    
    success = True
    
    # Step 1: Check Python version
    if not check_python_version():
        success = False
    
    # Step 2: Update OpenAI package
    if not update_openai_package():
        success = False
    
    # Step 3: Check Ollama
    ollama_ok = check_ollama_installation()
    if ollama_ok:
        ensure_basic_models()
    
    # Step 4: Update config
    if not update_config_file():
        success = False
    
    # Step 5: Test integration
    if not test_integration():
        success = False
    
    print("\n" + "=" * 60)
    if success:
        print("‚úÖ LLM INTEGRATION FIXED SUCCESSFULLY!")
        print("\nüí° Next steps:")
        print("   ‚Ä¢ Test with: python main.py --claim 'Test claim'")
        print("   ‚Ä¢ Check results.json for output")
        print("   ‚Ä¢ Monitor logs for LLM usage")
    else:
        print("‚ö†Ô∏è  SOME ISSUES REMAIN")
        print("\nüîß Manual steps you may need:")
        print("   ‚Ä¢ Install Ollama: https://ollama.ai/download")
        print("   ‚Ä¢ Pull a model: ollama pull llama2")
        print("   ‚Ä¢ Start Ollama: ollama serve")
        print("   ‚Ä¢ Add OpenAI API key to config if needed")
    
    print("\nüìã System Status Summary:")
    print(f"   ‚Ä¢ Python: {'‚úÖ' if sys.version_info >= (3, 8) else '‚ùå'}")
    print(f"   ‚Ä¢ OpenAI Package: {'‚úÖ' if success else '‚ùå'}")
    print(f"   ‚Ä¢ Ollama: {'‚úÖ' if ollama_ok else '‚ùå'}")
    print(f"   ‚Ä¢ Integration: {'‚úÖ' if success else '‚ö†Ô∏è'}")

if __name__ == "__main__":
    main()